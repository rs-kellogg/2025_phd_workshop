{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7135220",
   "metadata": {},
   "source": [
    "#  LLM Testing \n",
    "\n",
    "## LLM Testing List\n",
    "- Structure & Formatting\n",
    "  * Length control: Ensure response respect token limits.\n",
    "  * Output structure enforcement: Validate that responses follow required formats (e.g., bullet points, tables, JSON).\n",
    "\n",
    "- Correctness: Check against expected answers or external source (exact match or similarity)\n",
    "\n",
    "- Consistency & Stability\n",
    "  * Reproducibility: Set temperature and seed value.\n",
    "  * Prompt sensitivity: Assess how changes in wording affect results.\n",
    "  * Regression testing: Detect output changes over model versions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b4c6a",
   "metadata": {},
   "source": [
    "\n",
    "### Case 1: Extract infrmation from earning report\n",
    "* Defines a \"golden set\" of expected phrases (like unit test assertions).\n",
    "* Checks that the LLM includes all critical facts.\n",
    "* Gives a clean pass/fail result with explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6df9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def llm_openai(prompt: str, llm_model: str, temperature, max_tokens) -> str:\n",
    "    \"\"\"Call OpenAI ChatCompletion API and return output text.\"\"\"\n",
    "\n",
    "    # Set your OpenAI API key as an environment variable before running\n",
    "    api_key = Path(\"./openai.key\").read_text().strip()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_tokens,\n",
    "        seed=42,\n",
    "    )   \n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa21474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "def llm_with_logging(prompt, llm_model, llm_func, temperature, max_tokens):\n",
    "    \"\"\"\n",
    "    Executes an LLM prompt using a provided function, saves the prompt and logs the interaction.\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the LLM.\n",
    "        llm_model (str): The name of the LLM model to use.\n",
    "        llm_func (callable): The function to execute the LLM call. Defaults to llm_execute.\n",
    "    Returns:\n",
    "        str: The output from the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    log_dir = \"logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    date_str = now.date().isoformat()\n",
    "    timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    output_file = os.path.join(log_dir, f\"logfile_{date_str}.jsonl\")\n",
    "\n",
    "    output = llm_func(prompt, llm_model, temperature, max_tokens)\n",
    "\n",
    "    log_entry = {\n",
    "        \"timestamp\": now.isoformat(),\n",
    "        \"model\": llm_model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"prompt\": prompt,\n",
    "        \"output\": output,\n",
    "    }\n",
    "\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "    print(f\"Logged interaction to {output_file}\")\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be615e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summary_quality(output: str, expected_phrases: List[str]) -> bool:\n",
    "    \"\"\"Unit-style test: check if all expected phrases appear in the output.\"\"\"\n",
    "    output = output.lower()\n",
    "    expected_phrases = [phrase.lower() for phrase in expected_phrases]\n",
    "    missing = [phrase for phrase in expected_phrases if phrase not in output]\n",
    "    if missing:\n",
    "        print(\"=== Test failed. Missing key points:\")\n",
    "        for m in missing:\n",
    "            print(f\" - {m}\")\n",
    "        return False\n",
    "    print(\"=== Test passed. Output contains all expected key points.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5519ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "\n",
      "=== LLM Output:\n",
      " Q1 2024 Earnings Summary for Internal Strategy Briefing:\n",
      "\n",
      "**Revenue:**  \n",
      "Starbucks generated $9.4 billion in net revenues, up 8% year-over-year, reflecting solid growth across key markets.\n",
      "\n",
      "**Profit:**  \n",
      "Net income increased to $1.1 billion ($0.78 per share) from previous earnings of $0.68 per share in Q1 2023, indicating improved profitability.\n",
      "\n",
      "**Growth Drivers:**  \n",
      "- **Domestic Market:** North America experienced a 9% revenue increase driven by higher average tickets and increased store traffic.  \n",
      "- **International Markets:** International revenue grew 7%, with China leading the way, where same-store sales surged 11%.  \n",
      "- **Expansion:** The company opened 549 new stores globally, reaching over 38,000 locations.  \n",
      "- **Digital Investment:** Continued focus on digital platforms and mobile ordering has strengthened competitive positioning.\n",
      "\n",
      "**Geographic Performance:**  \n",
      "- **North America:** Strong growth with a 9% revenue increase.  \n",
      "- **International:** 7% growth overall, with notable strength in China (11% same-store sales growth).\n",
      "\n",
      "**Margins & Outlook:**  \n",
      "Operating margin expanded by 40 basis points year-over-year. Starbucks reaffirmed its full-year guidance of high-single-digit revenue growth, emphasizing ongoing strategic investments.\n",
      "\n",
      "=== Running Unit Test...\n",
      "\n",
      "=== Test passed. Output contains all expected key points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Realistic business prompt for LLM ---\n",
    "test_prompt = \"\"\"\n",
    "Summarize the following Q1 2024 earnings report for internal strategy briefings. Focus on revenue, profit, growth drivers, and geographic performance:\n",
    "\n",
    "'In its first quarter of fiscal 2024, Starbucks reported consolidated net revenues of $9.4 billion, representing an 8% increase over the prior year. Net income grew to $1.1 billion, or $0.78 per share, compared to $0.68 per share in Q1 2023. The North America segment saw a 9% revenue increase, fueled by higher average ticket and increased store traffic. International revenue rose 7%, with particularly strong performance in China where same-store sales jumped 11%. Starbucks opened 549 new stores globally in the quarter, bringing its total to over 38,000. CEO Laxman Narasimhan cited continued investment in digital platforms and mobile ordering as a key competitive advantage. Operating margin expanded 40 basis points year-over-year. The company reaffirmed its full-year guidance of high-single-digit revenue growth.'\n",
    "\"\"\"\n",
    "\n",
    "# --- Define expected content for unit test ---\n",
    "expected_phrases = [\n",
    "    \"$9.4 billion\",\n",
    "    \"$1.1 billion\",\n",
    "    \"North America\",\n",
    "    \"China\",\n",
    "    # \"549 new stores\",\n",
    "    # \"digital platforms and mobile ordering\",\n",
    "    # \"operating margin\"\n",
    "]\n",
    "\n",
    "# --- Run the test ---\n",
    "llm_model = \"gpt-4.1-nano\"\n",
    "temperature = 0.7\n",
    "max_tokens = 1000\n",
    "output = llm_with_logging(test_prompt, llm_model, llm_openai,temperature, max_tokens)\n",
    "print(\"\\n=== LLM Output:\\n\", output)\n",
    "print(\"\\n=== Running Unit Test...\\n\")\n",
    "test_summary_quality(output, expected_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0704550",
   "metadata": {},
   "source": [
    "### Case 2: Generate marketing materials\n",
    "\n",
    "* Track changes in model output over time or when prompts/models change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e31f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "\n",
      "=== LLM Output:\n",
      " Unlock Your Adventure: Earn Travel Perks, Cash Back, and No Annual Fee with Our New Rewards Credit Card!\n",
      "=== Test passed. Output contains all expected key points.\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "\n",
      "=== LLM Output:\n",
      " \"Unlock Your Next Adventure: Introducing the Ultimate Rewards Credit Card for Young Professionals\"\n",
      "=== Test failed. Missing key points:\n",
      " - travel\n",
      " - no annual fee\n",
      " - cash back\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = \"\"\"Write a compelling headline for an email marketing campaign promoting a new rewards credit card for young professionals. Focus on travel perks, no annual fee, and cash back.\"\"\"\n",
    "\n",
    "expected_phrases = [\"travel\", \"no annual fee\", \"cash back\"]\n",
    "\n",
    "# Run on two model versions\n",
    "llm_model = \"gpt-4.1-nano\"\n",
    "temperature = 0.7\n",
    "max_tokens = 1000\n",
    "output = llm_with_logging(test_prompt, llm_model, llm_openai, temperature, max_tokens)\n",
    "print(\"\\n=== LLM Output:\\n\", output)\n",
    "test_summary_quality(output, expected_phrases)\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "output = llm_with_logging(test_prompt, llm_model, llm_openai, temperature, max_tokens)\n",
    "print(\"\\n=== LLM Output:\\n\", output)\n",
    "test_summary_quality(output, expected_phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcba843",
   "metadata": {},
   "source": [
    "### Case 3: Check classification outcome\n",
    "\n",
    "* Adopt embedding to compare similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e2ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "def get_embedding_openai(text):\n",
    "    \"\"\"\n",
    "    Get embedding vector for a category string using OpenAI embeddings API.\n",
    "    \"\"\"\n",
    "    api_key = Path(\"./openai.key\").read_text().strip()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Use a small model for speed/cost; adjust as needed\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2953a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#  Evaluation Function\n",
    "def evaluate_product_classification(df, llm_model, prompt, temperature, max_tokens):\n",
    "    \"\"\"\n",
    "    Send each product name to LLM and compare output category to golden label.\n",
    "    Adds columns for LLM response, predicted category, match status, and error type.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        product = row[\"product_name\"]\n",
    "        gold_category = row[\"ground_truth_category\"]\n",
    "\n",
    "        user_prompt = f\"{prompt} {product}\\n\"\n",
    "\n",
    "        llm_output = llm_with_logging(user_prompt, llm_model, llm_openai, temperature, max_tokens)\n",
    "\n",
    "        # Try to extract category from LLM output\n",
    "        category_match = re.search(r\"Category:\\s*(.*)\", llm_output)\n",
    "        predicted_category = category_match.group(1).strip().lower() if category_match else \"N/A\"\n",
    "\n",
    "        match = predicted_category == gold_category.lower()\n",
    "\n",
    "        # Get embeddings and similarity\n",
    "        pred_emb = get_embedding_openai(predicted_category)\n",
    "        gold_emb = get_embedding_openai(gold_category)\n",
    "        similarity = cosine_similarity(pred_emb, gold_emb)\n",
    "\n",
    "        results.append({\n",
    "            \"product_name\": product,\n",
    "            \"gold_category\": gold_category,\n",
    "            \"llm_response\": llm_output,\n",
    "            \"predicted_category\": predicted_category,\n",
    "            \"match\": match,\n",
    "            \"error_type\": \"\" if match else \"misclassification\",\n",
    "            \"cosine_similarity\": similarity\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de0f074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n",
      "Logged interaction to logs/logfile_2025-07-01.jsonl\n"
     ]
    }
   ],
   "source": [
    "df_products = pd.DataFrame({\n",
    "    \"product_name\": [\n",
    "        \"Dyson V11 Torque Drive\",\n",
    "        \"Keurig K-Supreme Plus SMART\",\n",
    "        \"iPhone 15 Pro Max\",\n",
    "        \"YETI Rambler 20 oz Tumbler\",\n",
    "        \"Sony WH-1000XM5\",\n",
    "        \"Blue Diamond Almonds - Lightly Salted\",\n",
    "        \"Peloton Bike+\",\n",
    "        \"Nest Thermostat (3rd Gen)\",\n",
    "        \"L'Oréal Paris Revitalift Serum\",\n",
    "        \"Kindle Paperwhite Signature Edition\"\n",
    "    ],\n",
    "    \"ground_truth_category\": [\n",
    "        \"vacuum cleaner\",\n",
    "        \"coffee maker\",\n",
    "        \"smartphone\",\n",
    "        \"drinkware\",\n",
    "        \"headphones\",\n",
    "        \"snack\",\n",
    "        \"exercise equipment\",\n",
    "        \"smart home device\",\n",
    "        \"skincare\",\n",
    "        \"e-reader\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "test_prompt = \"\"\"Classify the following product into a specific consumer product category. \n",
    "Reply in the format: \n",
    "Category: <your category>\n",
    "Reason: <your reasoning based on the product name>\n",
    "Product: \"\"\"\n",
    "\n",
    "llm_model = \"gpt-4.1-nano\"\n",
    "temperature = 0.7\n",
    "max_tokens = 1000\n",
    "df_results = evaluate_product_classification(df_products, llm_model, test_prompt, temperature, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0209c1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            product_name  \\\n",
      "0                 Dyson V11 Torque Drive   \n",
      "1            Keurig K-Supreme Plus SMART   \n",
      "2                      iPhone 15 Pro Max   \n",
      "3             YETI Rambler 20 oz Tumbler   \n",
      "4                        Sony WH-1000XM5   \n",
      "5  Blue Diamond Almonds - Lightly Salted   \n",
      "6                          Peloton Bike+   \n",
      "7              Nest Thermostat (3rd Gen)   \n",
      "8         L'Oréal Paris Revitalift Serum   \n",
      "9    Kindle Paperwhite Signature Edition   \n",
      "\n",
      "                     predicted_category       gold_category  match  \\\n",
      "0                       vacuum cleaners      vacuum cleaner  False   \n",
      "1               small kitchen appliance        coffee maker  False   \n",
      "2                            smartphone          smartphone   True   \n",
      "3                  drinkware / tumblers           drinkware  False   \n",
      "4      headphones / wireless headphones          headphones  False   \n",
      "5                            snack food               snack  False   \n",
      "6                     fitness equipment  exercise equipment  False   \n",
      "7  home automation / smart home devices   smart home device  False   \n",
      "8              skincare / facial serums            skincare  False   \n",
      "9     e-reader / electronic book reader            e-reader  False   \n",
      "\n",
      "   cosine_similarity  \n",
      "0           0.894264  \n",
      "1           0.519339  \n",
      "2           0.999999  \n",
      "3           0.821989  \n",
      "4           0.788420  \n",
      "5           0.900429  \n",
      "6           0.909726  \n",
      "7           0.795663  \n",
      "8           0.737094  \n",
      "9           0.882975  \n"
     ]
    }
   ],
   "source": [
    "df_results.to_csv(\"product_classification_results.csv\", index=False)\n",
    "print(df_results[[\"product_name\", \"predicted_category\", \"gold_category\", \"match\",\"cosine_similarity\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
