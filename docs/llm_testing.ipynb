{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7135220",
   "metadata": {},
   "source": [
    "#  Workshop Module: LLM Testing Strategies for Business Research\n",
    "\n",
    "## LLM Testing\n",
    "- Correctness & Accuracy\n",
    "  * **Unit tests**: Check exact match against expected answers.\n",
    "  * **Classification metrics**: Compare predicted labels to ground truth (e.g. accuracy, F1).\n",
    "  * **Factuality checks**: Validate claims against external sources; flag hallucinations.\n",
    "\n",
    "- Consistency & Stability\n",
    "  * **Regression testing**: Detect output changes over time or between model versions.\n",
    "  * **Prompt sensitivity**: Assess how small wording or formatting changes affect results.\n",
    "  * **Reproducibility**: Ensure the same prompt yields the same output under controlled conditions.\n",
    "\n",
    "- Structure & Formatting\n",
    "  * **Output structure enforcement**: Validate that responses follow required formats (e.g., bullet points, tables, JSON).\n",
    "  * **Length control**: Ensure response respect word limits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b4c6a",
   "metadata": {},
   "source": [
    "\n",
    "### Case 1: Extract infrmation from earning report\n",
    "* Defines a \"golden set\" of expected phrases (like unit test assertions).\n",
    "* Checks that the LLM includes all critical facts.\n",
    "* Gives a clean pass/fail result with explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6df9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def llm_openai(prompt: str, llm_model: str) -> str:\n",
    "    \"\"\"Call OpenAI ChatCompletion API and return output text.\"\"\"\n",
    "\n",
    "    # Set your OpenAI API key as an environment variable before running\n",
    "    api_key = Path(\"./openai.key\").read_text().strip()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )   \n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "def llm_with_logging(prompt, llm_model, llm_func):\n",
    "    \"\"\"\n",
    "    Executes an LLM prompt using a provided function, saves the prompt and logs the interaction.\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the LLM.\n",
    "        llm_model (str): The name of the LLM model to use.\n",
    "        llm_func (callable): The function to execute the LLM call. Defaults to llm_execute.\n",
    "    Returns:\n",
    "        str: The output from the LLM.\n",
    "    \"\"\"\n",
    "    log_dir = \"logs\"\n",
    "    # prompts_dir = \"prompts\"\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    # os.makedirs(prompts_dir, exist_ok=True)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    date_str = now.date().isoformat()\n",
    "    timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    output_file = os.path.join(log_dir, f\"logfile_{date_str}.jsonl\")\n",
    "    # prompt_file = os.path.join(prompts_dir, f\"prompt_{timestamp_str}.txt\")\n",
    "\n",
    "    # with open(prompt_file, \"w\") as f:\n",
    "    #     f.write(prompt)\n",
    "    # print(f\"Saved prompt to {prompt_file}\")\n",
    "\n",
    "    output = llm_func(prompt, llm_model)\n",
    "\n",
    "    log_entry = {\n",
    "        \"timestamp\": now.isoformat(),\n",
    "        \"model\": llm_model,\n",
    "        \"prompt\": prompt,\n",
    "        \"output\": output,\n",
    "    }\n",
    "\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "    print(f\"Logged interaction to {output_file}\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be615e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summary_quality(output: str, expected_phrases: List[str]) -> bool:\n",
    "    \"\"\"Unit-style test: check if all expected phrases appear in the output.\"\"\"\n",
    "    output = output.lower()\n",
    "    expected_phrases = [phrase.lower() for phrase in expected_phrases]\n",
    "    missing = [phrase for phrase in expected_phrases if phrase not in output]\n",
    "    if missing:\n",
    "        print(\"=== Test failed. Missing key points:\")\n",
    "        for m in missing:\n",
    "            print(f\" - {m}\")\n",
    "        return False\n",
    "    print(\"=== Test passed. Output contains all expected key points.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b5519ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prompt to prompts/prompt_20250618-144651.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "\n",
      "üì§ LLM Output:\n",
      " Q1 2024 Earnings Summary for Internal Strategy Briefing:\n",
      "\n",
      "**Revenue and Profit:**\n",
      "- Consolidated net revenues reached $9.4 billion, up 8% year-over-year.\n",
      "- Net income increased to $1.1 billion, with EPS of $0.78, compared to $0.68 in Q1 2023.\n",
      "- Operating margin expanded by 40 basis points, indicating improved profitability.\n",
      "\n",
      "**Growth Drivers:**\n",
      "- Strong performance driven by higher average tickets and increased store traffic, particularly in North America.\n",
      "- Continued investment in digital platforms and mobile ordering enhanced customer engagement and sales.\n",
      "- Opening of 549 new stores globally, expanding the total footprint to over 38,000 locations.\n",
      "\n",
      "**Geographic Performance:**\n",
      "- North America: Revenue grew 9%, supported by increased customer visits and higher spend per visit.\n",
      "- International: Revenue rose 7%, with China leading international growth‚Äîsame-store sales up 11%, highlighting robust market performance.\n",
      "\n",
      "**Outlook:**\n",
      "- The company reaffirmed its full-year guidance, targeting high-single-digit revenue growth.\n",
      "\n",
      "This performance underscores the effectiveness of strategic investments in digital channels and store expansion, with strong growth momentum in key markets, especially China.\n",
      "\n",
      "üîç Running Unit Test...\n",
      "\n",
      "=== Test passed. Output contains all expected key points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Realistic business prompt for LLM ---\n",
    "test_prompt = \"\"\"\n",
    "Summarize the following Q1 2024 earnings report for internal strategy briefings. Focus on revenue, profit, growth drivers, and geographic performance:\n",
    "\n",
    "'In its first quarter of fiscal 2024, Starbucks reported consolidated net revenues of $9.4 billion, representing an 8% increase over the prior year. Net income grew to $1.1 billion, or $0.78 per share, compared to $0.68 per share in Q1 2023. The North America segment saw a 9% revenue increase, fueled by higher average ticket and increased store traffic. International revenue rose 7%, with particularly strong performance in China where same-store sales jumped 11%. Starbucks opened 549 new stores globally in the quarter, bringing its total to over 38,000. CEO Laxman Narasimhan cited continued investment in digital platforms and mobile ordering as a key competitive advantage. Operating margin expanded 40 basis points year-over-year. The company reaffirmed its full-year guidance of high-single-digit revenue growth.'\n",
    "\"\"\"\n",
    "\n",
    "# --- Define expected content for unit test ---\n",
    "expected_phrases = [\n",
    "    \"$9.4 billion\",\n",
    "    \"$1.1 billion\",\n",
    "    \"North America\",\n",
    "    \"China\",\n",
    "    # \"549 new stores\",\n",
    "    # \"digital platforms and mobile ordering\",\n",
    "    # \"operating margin\"\n",
    "]\n",
    "\n",
    "# --- Run the test ---\n",
    "llm_model = \"gpt-4.1-nano\"\n",
    "output = llm_with_logging(test_prompt, llm_model, llm_openai)\n",
    "print(\"\\nüì§ LLM Output:\\n\", output)\n",
    "print(\"\\nüîç Running Unit Test...\\n\")\n",
    "test_summary_quality(output, expected_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0704550",
   "metadata": {},
   "source": [
    "### Case 2: Generate marketing materials\n",
    "\n",
    "* Track changes in model output over time or when prompts/models change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e31f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prompt to prompts/prompt_20250618-144653.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "\n",
      "üì§ LLM Output:\n",
      " Unlock Your Adventure: No Fee, Big Rewards ‚Äî Travel Perks & Cash Back for Young Professionals!\n",
      "=== Test failed. Missing key points:\n",
      " - no annual fee\n",
      "Saved prompt to prompts/prompt_20250618-144653.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "\n",
      "üì§ LLM Output:\n",
      " \"Unlock Your Passport to Adventure with Our New Rewards Credit Card - No Annual Fee, Cash Back, and Travel Perks Await!\"\n",
      "=== Test passed. Output contains all expected key points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = \"\"\"Write a compelling headline for an email marketing campaign promoting a new rewards credit card for young professionals. Focus on travel perks, no annual fee, and cash back.\"\"\"\n",
    "\n",
    "expected_phrases = [\"travel\", \"no annual fee\", \"cash back\"]\n",
    "\n",
    "# Run on two model versions\n",
    "llm_model = \"gpt-4.1-nano\"\n",
    "output = llm_with_logging(test_prompt, llm_model, llm_openai)\n",
    "print(\"\\nüì§ LLM Output:\\n\", output)\n",
    "test_summary_quality(output, expected_phrases)\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "output = llm_with_logging(test_prompt, llm_model, llm_openai)\n",
    "print(\"\\nüì§ LLM Output:\\n\", output)\n",
    "test_summary_quality(output, expected_phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcba843",
   "metadata": {},
   "source": [
    "### Case 3: Check classification outcome\n",
    "\n",
    "* Adopt embedding to compare similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12e2ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "def get_embedding_openai(text):\n",
    "    \"\"\"\n",
    "    Get embedding vector for a category string using OpenAI embeddings API.\n",
    "    \"\"\"\n",
    "    api_key = Path(\"./openai.key\").read_text().strip()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Use a small model for speed/cost; adjust as needed\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc2953a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#  Evaluation Function\n",
    "def evaluate_product_classification(df, llm_model, prompt):\n",
    "    \"\"\"\n",
    "    Send each product name to LLM and compare output category to golden label.\n",
    "    Adds columns for LLM response, predicted category, match status, and error type.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        product = row[\"product_name\"]\n",
    "        gold_category = row[\"ground_truth_category\"]\n",
    "\n",
    "        user_prompt = f\"{prompt} {product}\\n\"\n",
    "\n",
    "        llm_output = llm_with_logging(user_prompt, llm_model, llm_openai)\n",
    "\n",
    "        # Try to extract category from LLM output\n",
    "        category_match = re.search(r\"Category:\\s*(.*)\", llm_output)\n",
    "        predicted_category = category_match.group(1).strip().lower() if category_match else \"N/A\"\n",
    "\n",
    "        match = predicted_category == gold_category.lower()\n",
    "\n",
    "        # Get embeddings and similarity\n",
    "        pred_emb = get_embedding_openai(predicted_category)\n",
    "        gold_emb = get_embedding_openai(gold_category)\n",
    "        similarity = cosine_similarity(pred_emb, gold_emb)\n",
    "\n",
    "        results.append({\n",
    "            \"product_name\": product,\n",
    "            \"gold_category\": gold_category,\n",
    "            \"llm_response\": llm_output,\n",
    "            \"predicted_category\": predicted_category,\n",
    "            \"match\": match,\n",
    "            \"error_type\": \"\" if match else \"misclassification\",\n",
    "            \"cosine_similarity\": similarity\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2de0f074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prompt to prompts/prompt_20250618-144654.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144655.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144657.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144658.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144659.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144700.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144702.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144703.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144706.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-144707.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n"
     ]
    }
   ],
   "source": [
    "df_products = pd.DataFrame({\n",
    "    \"product_name\": [\n",
    "        \"Dyson V11 Torque Drive\",\n",
    "        \"Keurig K-Supreme Plus SMART\",\n",
    "        \"iPhone 15 Pro Max\",\n",
    "        \"YETI Rambler 20 oz Tumbler\",\n",
    "        \"Sony WH-1000XM5\",\n",
    "        \"Blue Diamond Almonds - Lightly Salted\",\n",
    "        \"Peloton Bike+\",\n",
    "        \"Nest Thermostat (3rd Gen)\",\n",
    "        \"L'Or√©al Paris Revitalift Serum\",\n",
    "        \"Kindle Paperwhite Signature Edition\"\n",
    "    ],\n",
    "    \"ground_truth_category\": [\n",
    "        \"vacuum cleaner\",\n",
    "        \"coffee maker\",\n",
    "        \"smartphone\",\n",
    "        \"drinkware\",\n",
    "        \"headphones\",\n",
    "        \"snack\",\n",
    "        \"exercise equipment\",\n",
    "        \"smart home device\",\n",
    "        \"skincare\",\n",
    "        \"e-reader\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "test_prompt = \"\"\"Classify the following product into a specific consumer product category. \n",
    "Reply in the format: \n",
    "Category: <your category>\n",
    "Reason: <your reasoning based on the product name>\n",
    "Product: \"\"\"\n",
    "\n",
    "llm_model = \"gpt-4.1-nano\"\n",
    "df_results = evaluate_product_classification(df_products, llm_model, test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0209c1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            product_name  \\\n",
      "0                 Dyson V11 Torque Drive   \n",
      "1            Keurig K-Supreme Plus SMART   \n",
      "2                      iPhone 15 Pro Max   \n",
      "3             YETI Rambler 20 oz Tumbler   \n",
      "4                        Sony WH-1000XM5   \n",
      "5  Blue Diamond Almonds - Lightly Salted   \n",
      "6                          Peloton Bike+   \n",
      "7              Nest Thermostat (3rd Gen)   \n",
      "8         L'Or√©al Paris Revitalift Serum   \n",
      "9    Kindle Paperwhite Signature Edition   \n",
      "\n",
      "                     predicted_category       gold_category  match  \\\n",
      "0                       vacuum cleaners      vacuum cleaner  False   \n",
      "1              small kitchen appliances        coffee maker  False   \n",
      "2                            smartphone          smartphone   True   \n",
      "3                  drinkware / tumblers           drinkware  False   \n",
      "4          headphones / audio equipment          headphones  False   \n",
      "5                      food & beverages               snack  False   \n",
      "6                     fitness equipment  exercise equipment  False   \n",
      "7  home automation / smart home devices   smart home device  False   \n",
      "8              skincare / facial serums            skincare  False   \n",
      "9     e-reader / electronic book reader            e-reader  False   \n",
      "\n",
      "   cosine_similarity  \n",
      "0           0.894336  \n",
      "1           0.485816  \n",
      "2           0.999999  \n",
      "3           0.822056  \n",
      "4           0.794836  \n",
      "5           0.413488  \n",
      "6           0.909726  \n",
      "7           0.795683  \n",
      "8           0.736923  \n",
      "9           0.882946  \n"
     ]
    }
   ],
   "source": [
    "df_results.to_csv(\"product_classification_results.csv\", index=False)\n",
    "print(df_results[[\"product_name\", \"predicted_category\", \"gold_category\", \"match\",\"cosine_similarity\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
