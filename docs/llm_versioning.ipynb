{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9fc127",
   "metadata": {},
   "source": [
    "# Workshop Module: Logging & Version Control for Computational Research\n",
    "\n",
    "## Part 1: Logging Your Work \n",
    "\n",
    "### Why Logging Matters\n",
    "\n",
    "* Reproducibility: What prompt did I use?\n",
    "* Accountability: When did I run this?\n",
    "* Version tracking: What changed since last run?\n",
    "\n",
    "###  Example Task: Log a Simple LLM Prompt\n",
    "\n",
    "- Save log files in a `./logs` folder by date.\n",
    "- This appends a log line to `./logs/logfile_YYYY-MM-DD.jsonl`.\n",
    "- Use `.jsonl` (JSON Lines) format for logging experiments, easy to parse later\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    df = pd.read_json(\"log.jsonl\", lines=True)\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e47aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def llm_openai(prompt: str, llm_model: str) -> str:\n",
    "    \"\"\"Call OpenAI ChatCompletion API and return output text.\"\"\"\n",
    "\n",
    "    # Set your OpenAI API key as an environment variable before running\n",
    "    api_key = Path(\"./openai.key\").read_text().strip()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )   \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def llm_faketest(prompt: str, llm_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes a prompt against a specified LLM model.\n",
    "    This is a placeholder for a real API call.\n",
    "    \"\"\"\n",
    "    print(f\"Executing prompt with model: {llm_model}\")\n",
    "    return f\"Fake output for: {prompt}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "def llm_with_logging(prompt, llm_model, llm_func):\n",
    "    \"\"\"\n",
    "    Executes an LLM prompt using a provided function, saves the prompt and logs the interaction.\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the LLM.\n",
    "        llm_model (str): The name of the LLM model to use.\n",
    "        llm_func (callable): The function to execute the LLM call. Defaults to llm_execute.\n",
    "    Returns:\n",
    "        str: The output from the LLM.\n",
    "    \"\"\"\n",
    "    log_dir = \"logs\"\n",
    "    prompts_dir = \"prompts\"\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(prompts_dir, exist_ok=True)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    date_str = now.date().isoformat()\n",
    "    timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    output_file = os.path.join(log_dir, f\"logfile_{date_str}.jsonl\")\n",
    "    prompt_file = os.path.join(prompts_dir, f\"prompt_{timestamp_str}.txt\")\n",
    "\n",
    "    with open(prompt_file, \"w\") as f:\n",
    "        f.write(prompt)\n",
    "    print(f\"Saved prompt to {prompt_file}\")\n",
    "\n",
    "    output = llm_func(prompt, llm_model)\n",
    "\n",
    "    log_entry = {\n",
    "        \"timestamp\": now.isoformat(),\n",
    "        \"model\": llm_model,\n",
    "        \"prompt\": prompt,\n",
    "        \"output\": output,\n",
    "    }\n",
    "\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "    print(f\"Logged interaction to {output_file}\")\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b64ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prompt to prompts/prompt_20250618-105847.txt\n",
      "Executing prompt with model: test-model\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n",
      "Saved prompt to prompts/prompt_20250618-105847.txt\n",
      "Logged interaction to logs/logfile_2025-06-18.jsonl\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Tell a quick fun fact about business.\"\n",
    "llm_model = \"gpt-4.1-nano\"\n",
    "output = llm_with_logging(test_prompt, \"test-model\", llm_faketest)\n",
    "output = llm_with_logging(test_prompt, llm_model, llm_openai)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655dce1",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Version Control with Git \n",
    "\n",
    "### Why Use Git\n",
    "\n",
    "* Track code and prompt changes over time\n",
    "* Collaborate with co-authors or advisors\n",
    "* Restore earlier versions\n",
    "\n",
    "### Quick Git Demo\n",
    "\n",
    "- **Step 1**: Initialize a Git repo\n",
    "\n",
    "    ```bash\n",
    "    git init llm-project\n",
    "    cd llm-project\n",
    "    ```\n",
    "\n",
    "- **Step 2**: Add your script\n",
    "\n",
    "    ```bash\n",
    "    cp ../log_llm_prompt.py .\n",
    "    git add log_llm_prompt.py\n",
    "    git commit -m \"Initial LLM logging script\"\n",
    "    ```\n",
    "\n",
    "- **Step 3**: Modify your prompt and commit again\n",
    "    Edit the `prompt` line in `log_llm_prompt.py`, then:\n",
    "\n",
    "    ```bash\n",
    "    git diff                # See what changed\n",
    "    git add log_llm_prompt.py\n",
    "    git commit -m \"Updated prompt to focus on financial trends\"\n",
    "    ```\n",
    "\n",
    "- **Step 4**: View history\n",
    "    ```bash\n",
    "    git log --oneline\n",
    "    ```\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "* Log every prompt and output with timestamps and model version\n",
    "* Use Git to track changes in your code and prompts\n",
    "* These practices make your research more reproducible, especially when using LLMs or modeling pipelines\n",
    "\n",
    "\n",
    "* Include model version/hash in logs (for real models like OpenAI or Claude)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
